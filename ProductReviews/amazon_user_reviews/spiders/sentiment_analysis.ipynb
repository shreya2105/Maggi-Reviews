{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this workbook, sentiment of reviews will be measured using the stars and then the text of the review. \n",
    "#The sentiment will be mapped with dates and be visualized through three years.\n",
    "#Cluster analysis - topic modeling - What are people talking about?\n",
    "# What are my questions?\n",
    "\n",
    "#Q. In last three years, what are the most popular words that people have used to describe maggie?\n",
    "#Q. How has the sentiment evolved over a period of three years?\n",
    "#Q. \n",
    "\n",
    "#Possible topics - taste, texture, packaging, time of arrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the cleaned csv\n",
    "\n",
    "maggie = pd.read_csv(\"maggie.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stars</th>\n",
       "      <th>comment</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Family fun pack! They supposedly forgot to men...</td>\n",
       "      <td>2018-11-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The product is expired or something is very wr...</td>\n",
       "      <td>2019-01-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>This product was supplied in damaged condition...</td>\n",
       "      <td>2017-11-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Can you use a product when it is tasted by mou...</td>\n",
       "      <td>2019-09-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Maggi is called national food of India, lol. I...</td>\n",
       "      <td>2019-06-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stars                                            comment        date\n",
       "0      1  Family fun pack! They supposedly forgot to men...  2018-11-02\n",
       "1      1  The product is expired or something is very wr...  2019-01-16\n",
       "2      1  This product was supplied in damaged condition...  2017-11-29\n",
       "3      1  Can you use a product when it is tasted by mou...  2019-09-13\n",
       "4      5  Maggi is called national food of India, lol. I...  2019-06-09"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maggie.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2020-06-22'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maggie[\"date\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2017-11-29'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maggie[\"date\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean text column -- remove stopwords, punctuation\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stopwords = nltk.corpus.stopwords.words('english')\n",
    "word_rooter = nltk.stem.snowball.PorterStemmer(ignore_stopwords=False).stem\n",
    "my_punctuation = '!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~â€¢@'\n",
    "exclusionList = ['maggie','product','would','kar','ki','noodles', 'sn', 'maggi', \"pack\", \"noodle\"]\n",
    "exclusions = '|'.join(exclusionList)\n",
    "\n",
    "# cleaning master function\n",
    "def clean_text(text, bigrams = False):\n",
    "    text = text.lower() # lower case\n",
    "    text = re.sub('['+my_punctuation + ']+', ' ', text) # strip punctuation\n",
    "    text = re.sub('\\s+', ' ', text) #remove double spacing\n",
    "    text = re.sub('([0-9]+)', '', text) # remove numbers\n",
    "    text = re.sub(exclusions, '', text) #remove common words like maggie and noodles\n",
    "    text_token_list = [word for word in text.split(' ')\n",
    "                            if word not in my_stopwords] # remove stopwords\n",
    "\n",
    "    text_token_list = [word_rooter(word) if '#' not in word else word\n",
    "                        for word in text_token_list] # apply word rooter\n",
    "    if bigrams:\n",
    "        text_token_list = text_token_list+[text_token_list[i]+'_'+text_token_list[i+1]\n",
    "                                            for i in range(len(text_token_list)-1)]\n",
    "    text = ' '.join(text_token_list)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "maggie['comment'] = maggie.comment.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# the vectorizer object will be used to transform text to vector form\n",
    "vectorizer = CountVectorizer(max_df=0.9, min_df = 25, token_pattern='\\w+|\\$[\\d\\.]+|\\S+')\n",
    "\n",
    "# apply transformation\n",
    "tf = vectorizer.fit_transform(maggie['comment']).toarray()\n",
    "\n",
    "# tf_feature_names tells us what word each column in the matric represents\n",
    "tf_feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "number_of_topics = 4\n",
    "\n",
    "model = LatentDirichletAllocation(n_components=number_of_topics, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='batch', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=10,\n",
       "                          mean_change_tol=0.001, n_components=4, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=0, topic_word_prior=None,\n",
       "                          total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    topic_dict = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_dict[\"Topic %d words\" % (topic_idx)]= ['{}'.format(feature_names[i])\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "        topic_dict[\"Topic %d weights\" % (topic_idx)]= ['{:.1f}'.format(topic[i])\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "    return pd.DataFrame(topic_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic 0 words</th>\n",
       "      <th>Topic 0 weights</th>\n",
       "      <th>Topic 1 words</th>\n",
       "      <th>Topic 1 weights</th>\n",
       "      <th>Topic 2 words</th>\n",
       "      <th>Topic 2 weights</th>\n",
       "      <th>Topic 3 words</th>\n",
       "      <th>Topic 3 weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>noodl</td>\n",
       "      <td>980.9</td>\n",
       "      <td>receiv</td>\n",
       "      <td>740.2</td>\n",
       "      <td>et</td>\n",
       "      <td>870.6</td>\n",
       "      <td>regular</td>\n",
       "      <td>850.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>good</td>\n",
       "      <td>808.0</td>\n",
       "      <td>tast</td>\n",
       "      <td>631.6</td>\n",
       "      <td>atta</td>\n",
       "      <td>849.5</td>\n",
       "      <td>review</td>\n",
       "      <td>849.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tast</td>\n",
       "      <td>697.4</td>\n",
       "      <td>ant</td>\n",
       "      <td>567.7</td>\n",
       "      <td>realli</td>\n",
       "      <td>849.2</td>\n",
       "      <td>noodl</td>\n",
       "      <td>695.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>522.5</td>\n",
       "      <td>famili</td>\n",
       "      <td>566.3</td>\n",
       "      <td>best</td>\n",
       "      <td>631.4</td>\n",
       "      <td>damag</td>\n",
       "      <td>568.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ag</td>\n",
       "      <td>458.5</td>\n",
       "      <td>groceri</td>\n",
       "      <td>566.2</td>\n",
       "      <td>rs</td>\n",
       "      <td>631.2</td>\n",
       "      <td>larg</td>\n",
       "      <td>566.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>like</td>\n",
       "      <td>349.2</td>\n",
       "      <td>minut</td>\n",
       "      <td>348.7</td>\n",
       "      <td>tast</td>\n",
       "      <td>630.9</td>\n",
       "      <td>plastic</td>\n",
       "      <td>566.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>differ</td>\n",
       "      <td>348.8</td>\n",
       "      <td>insid</td>\n",
       "      <td>283.9</td>\n",
       "      <td>cook</td>\n",
       "      <td>566.5</td>\n",
       "      <td>longer</td>\n",
       "      <td>566.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>make</td>\n",
       "      <td>348.6</td>\n",
       "      <td>condit</td>\n",
       "      <td>283.8</td>\n",
       "      <td>got</td>\n",
       "      <td>566.2</td>\n",
       "      <td>good</td>\n",
       "      <td>522.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>et</td>\n",
       "      <td>348.6</td>\n",
       "      <td>like</td>\n",
       "      <td>283.5</td>\n",
       "      <td>deliv</td>\n",
       "      <td>566.2</td>\n",
       "      <td>time</td>\n",
       "      <td>457.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>look</td>\n",
       "      <td>348.5</td>\n",
       "      <td>one</td>\n",
       "      <td>283.4</td>\n",
       "      <td>even</td>\n",
       "      <td>566.2</td>\n",
       "      <td>alway</td>\n",
       "      <td>348.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Topic 0 words Topic 0 weights Topic 1 words Topic 1 weights Topic 2 words  \\\n",
       "0         noodl           980.9        receiv           740.2            et   \n",
       "1          good           808.0          tast           631.6          atta   \n",
       "2          tast           697.4           ant           567.7        realli   \n",
       "3           min           522.5        famili           566.3          best   \n",
       "4            ag           458.5       groceri           566.2            rs   \n",
       "5          like           349.2         minut           348.7          tast   \n",
       "6        differ           348.8         insid           283.9          cook   \n",
       "7          make           348.6        condit           283.8           got   \n",
       "8            et           348.6          like           283.5         deliv   \n",
       "9          look           348.5           one           283.4          even   \n",
       "\n",
       "  Topic 2 weights Topic 3 words Topic 3 weights  \n",
       "0           870.6       regular           850.0  \n",
       "1           849.5        review           849.2  \n",
       "2           849.2         noodl           695.9  \n",
       "3           631.4         damag           568.0  \n",
       "4           631.2          larg           566.2  \n",
       "5           630.9       plastic           566.2  \n",
       "6           566.5        longer           566.2  \n",
       "7           566.2          good           522.7  \n",
       "8           566.2          time           457.6  \n",
       "9           566.2         alway           348.3  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_top_words = 10\n",
    "display_topics(model, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classify the sentiments --Positive and negative\n",
    "#Simple frequency analysis - Most popular words in reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stars are in the range of 1-5. Anything between below 3 will be considered a negative sentiment, 3 as neutral and \n",
    "#above 3 as positive.\n",
    "\n",
    "#Count #of 1's, 2's and 3's and 4's and 5's in ratings and create a separate cluster\n",
    "\n",
    "#Step 1: Create a new colummn labelling the sentiments as positive, negative and neutral\n",
    "maggie[\"sentiment\"] = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
